{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  transaction_id            timestamp  customer_id      city  product_id  \\\n",
      "0  TXN0000077275  2023-01-01 00:35:30  CUST0003778  Warszawa  PROD004174   \n",
      "1  TXN0000001975  2023-01-01 01:16:48  CUST0017124  Warszawa  PROD001943   \n",
      "2  TXN0000057581  2023-01-01 01:54:58  CUST0010032    Krakow  PROD001245   \n",
      "3  TXN0000072149  2023-01-01 03:02:28  CUST0013656     Radom  PROD001143   \n",
      "4  TXN0000019668  2023-01-01 04:18:18  CUST0016922  Warszawa  PROD000746   \n",
      "\n",
      "             category   price  quantity  total_amount  \n",
      "0  Artykuly_Spozywcze   30.23         2         60.46  \n",
      "1         Elektronika  318.12         1        318.12  \n",
      "2             Zabawki   59.10         5        295.50  \n",
      "3             Zabawki  101.40         2        202.80  \n",
      "4           Smartfony  426.73         2        853.46  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time \n",
    "\n",
    "df = pd.read_csv('wroshop_small.csv')\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grupowanie - Czas wykonania: 0.0108 sekundy\n",
      "Pamięć: 0.0012 MB\n",
      "Top10 - Czas wykonania: 0.0091 sekundy\n",
      "Pamięć: 0.0006 MB\n",
      "Filtrowanie - Czas wykonania: 0.0068 sekundy\n",
      "Pamięć: 1.4782 MB\n"
     ]
    }
   ],
   "source": [
    "# a)\n",
    "\n",
    "start_time = time.time()\n",
    "result = df.groupby('category')['total_amount'].sum()\n",
    "end_time = time.time()\n",
    "mem = result.memory_usage(deep=True) / 1024**2\n",
    "#print(result)\n",
    "print(f\"Grupowanie - Czas wykonania: {end_time - start_time:.4f} sekundy\")\n",
    "print(f\"Pamięć: {mem:.4f} MB\")\n",
    "\n",
    "\n",
    "start_time1 = time.time()\n",
    "result1 = df.groupby('city')['quantity'].sum().sort_values(ascending=False).head(10)\n",
    "end_time1 = time.time()\n",
    "mem1 = result1.memory_usage(deep=True) / 1024**2\n",
    "#print(result1)\n",
    "print(f\"Top10 - Czas wykonania: {end_time1 - start_time1:.4f} sekundy\")\n",
    "print(f\"Pamięć: {mem1:.4f} MB\")\n",
    "\n",
    "start_time2 = time.time()\n",
    "result2 = df[df['quantity'] > 6]\n",
    "end_time2 = time.time()\n",
    "mem2 = result2.memory_usage(deep=True).sum() / 1024**2\n",
    "#print(result2)\n",
    "print(f\"Filtrowanie - Czas wykonania: {end_time2 - start_time2:.4f} sekundy\")\n",
    "print(f\"Pamięć: {mem2:.4f} MB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grupowanie - Czas wykonania: 0.0767 sekundy\n",
      "Pamięć: 0.0012 MB\n",
      "Top10 - Czas wykonania: 0.0686 sekundy\n",
      "Pamięć: 0.0006 MB\n",
      "Filtrowanie - Czas wykonania: 0.0190 sekundy\n",
      "Pamięć: 14.9741 MB\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "# b)\n",
    "\n",
    "file_pattern = \"wroshop_medium/part_*.csv\"\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "files_sorted = sorted(files)\n",
    "\n",
    "df = pd.concat((pd.read_csv(file) for file in files_sorted), ignore_index=True)\n",
    "\n",
    "start_time = time.time()\n",
    "result = df.groupby('category')['total_amount'].sum()\n",
    "end_time = time.time()\n",
    "mem = result.memory_usage(deep=True) / 1024**2\n",
    "#print(result)\n",
    "print(f\"Grupowanie - Czas wykonania: {end_time - start_time:.4f} sekundy\")\n",
    "print(f\"Pamięć: {mem:.4f} MB\")\n",
    "\n",
    "\n",
    "start_time1 = time.time()\n",
    "result1 = df.groupby('city')['quantity'].sum().sort_values(ascending=False).head(10)\n",
    "end_time1 = time.time()\n",
    "mem1 = result1.memory_usage(deep=True) / 1024**2\n",
    "#print(result1)\n",
    "print(f\"Top10 - Czas wykonania: {end_time1 - start_time1:.4f} sekundy\")\n",
    "print(f\"Pamięć: {mem1:.4f} MB\")\n",
    "\n",
    "start_time2 = time.time()\n",
    "result2 = df[df['quantity'] > 6]\n",
    "end_time2 = time.time()\n",
    "mem2 = result2.memory_usage(deep=True).sum() / 1024**2\n",
    "#print(result2)\n",
    "print(f\"Filtrowanie - Czas wykonania: {end_time2 - start_time2:.4f} sekundy\")\n",
    "print(f\"Pamięć: {mem2:.4f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grupowanie - Czas wykonania: 2.3601 sekundy\n"
     ]
    }
   ],
   "source": [
    "# c)\n",
    "import dask.dataframe as dd\n",
    "import time\n",
    "\n",
    "ddf = dd.read_csv('wroshop_medium/*.csv')\n",
    "result = ddf.groupby('category')['total_amount'].sum()\n",
    "result.visualize(filename='graph.png')\n",
    "\n",
    "start_time = time.time()\n",
    "result.compute()\n",
    "end_time = time.time()\n",
    "print(f\"Grupowanie - Czas wykonania: {end_time - start_time:.4f} sekundy\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grupowanie - Czas wykonania: 0.0216 sekundy\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[category#192], functions=[avg(total_amount#195)])\n",
      "+- Exchange hashpartitioning(category#192, 200), ENSURE_REQUIREMENTS, [id=#139]\n",
      "   +- *(1) HashAggregate(keys=[category#192], functions=[partial_avg(total_amount#195)])\n",
      "      +- FileScan csv [category#192,total_amount#195] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/Users/kornelorawczak/Documents/studia/sem5/ml/list7/wroshop_medium], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<category:string,total_amount:double>\n",
      "\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/01 03:44:58 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 271492 ms exceeds timeout 120000 ms\n",
      "25/12/01 03:44:58 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder.appName(\"WroShop\").getOrCreate()\n",
    "df = spark.read.csv('wroshop_medium/', header = True, inferSchema = True)\n",
    "\n",
    "start_time = time.time()\n",
    "result = df.groupBy('category').avg('total_amount')\n",
    "end_time = time.time()\n",
    "print(f\"Grupowanie - Czas wykonania: {end_time - start_time:.4f} sekundy\")\n",
    "print(result.explain())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas:\n",
    "- Wszystkie dane muszą zmieścić się w pamięci RAM\n",
    "- Procesowanie na pojedynczym rdzeniu CPU\n",
    "- Brak dystrybucji - wszystko na jednej maszynie\n",
    "- Dobre dla małych danych, jedna maszyna i proste transformacje\n",
    "\n",
    "\n",
    "Dask:\n",
    "- Lazy evaluation - tworzy graf zadań\n",
    "- Dzieli dane na partycje\n",
    "- Wykonuje równolegle na wielu rdzeniach\n",
    "- Duże dane które nie mieszczą się na jednej maszynie i potrzeba wielordzeniowości w równoległych, niezależnych procesach\n",
    "\n",
    "Pyspark:\n",
    "- Dystrybuowane na wielu maszynach\n",
    "- Resilient Distributed Datasets (RDD) - odporne na błędy\n",
    "- Optimizer Catalyst - zaawansowana optymalizacja zapytań\n",
    "- Lazy evaluation z zaawansowanym planowaniem\n",
    "- Olbrzymie dane, zaawansowana optymalizacja"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_list2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
