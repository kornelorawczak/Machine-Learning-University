{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generowanie danych\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "num_trams = 200\n",
    "frequency = '1h'  # Odczyt co godzinę\n",
    "start_date = '2024-01-01'\n",
    "\n",
    "def generate_tram_data(days):\n",
    "    dates = pd.date_range(start=start_date, periods=days*24, freq=frequency)\n",
    "    data = []\n",
    "    \n",
    "    for tram_id in range(num_trams):\n",
    "        base_temp = np.random.normal(70, 5)\n",
    "        base_vibration = np.random.normal(4.5, 1)\n",
    "        base_speed = np.random.normal(35, 10)\n",
    "        \n",
    "        # Wzorce sezonowe\n",
    "        time_of_day = np.sin(2 * np.pi * np.arange(len(dates)) / 24)\n",
    "        day_of_week = np.sin(2 * np.pi * np.arange(len(dates)) / (24*7))\n",
    "        \n",
    "        for i, date in enumerate(dates):\n",
    "            # Normalne wahania wartości\n",
    "            temp = base_temp + \\\n",
    "                   time_of_day[i] * 8 + \\\n",
    "                   day_of_week[i] * 3 + \\\n",
    "                   np.random.normal(0, 2)\n",
    "            \n",
    "            vibration = base_vibration + \\\n",
    "                       time_of_day[i] * 1.2 + \\\n",
    "                       np.random.normal(0, 0.3)\n",
    "            \n",
    "            speed = np.clip(base_speed + \\\n",
    "                           time_of_day[i] * 15 + \\\n",
    "                           np.random.normal(0, 5), 0, 80)\n",
    "            \n",
    "            # Oznaczenie anomalii (0 = normalny, 1 = anomalia)\n",
    "            anomaly = 0\n",
    "            \n",
    "            # Symulacja awarii (5% tramwajów doświadczy awarii)\n",
    "            if tram_id < num_trams * 0.05:\n",
    "                # Punkt rozpoczęcia awarii\n",
    "                failure_start = np.random.randint(0, len(dates)-10)\n",
    "                \n",
    "                if i >= failure_start:\n",
    "                    # Postępująca awaria\n",
    "                    progress = (i - failure_start) / 10\n",
    "                    \n",
    "                    if progress < 0.3: # Faza wstępna\n",
    "                        temp += progress * 20\n",
    "                        vibration += progress * 3\n",
    "                    elif progress < 0.7:  # Faza krytyczna\n",
    "                        temp += 15 + progress * 30\n",
    "                        vibration += 2 + progress * 8\n",
    "                        speed *= (1 - progress*0.5)\n",
    "                        anomaly = 1\n",
    "                    else:  # Pełna awaria\n",
    "                        temp += 40 + progress * 50\n",
    "                        vibration += 10 + progress * 15\n",
    "                        speed = 0\n",
    "                        anomaly = 1\n",
    "\n",
    "            data.append({\n",
    "                'timestamp': date,\n",
    "                'tram_id': f'tram_{tram_id:03d}',\n",
    "                'temperature': max(temp, 20),\n",
    "                'vibration': max(vibration, 0),\n",
    "                'speed': max(speed, 0),\n",
    "                'anomaly': anomaly\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sensor_data (n_normal=180, n_anomaly=20, random_state=42):\n",
    "    \"\"\" Generuje dane czujnikow z anomaliami .\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    # Normalne tramwaje - 3 klastry operacyjne\n",
    "    normal = np . vstack ([\n",
    "        np.random.randn(60, 3) * 0.5 + [50, 10, 30], \n",
    "        np.random.randn(60, 3) * 0.5 + [55, 12, 35],\n",
    "        np.random.randn(60, 3) * 0.5 + [48, 8, 28]\n",
    "    ])\n",
    "    # Anomalie - nietypowe zachowania\n",
    "    anomalies = np.vstack ([\n",
    "        np.random.randn (10, 3) * 0.3 + [80, 25, 15], # przegrzanie\n",
    "        np.random.randn(10, 3) * 0.3 + [30, 30, 5] # awaria silnika\n",
    "    ])\n",
    "    X = np.vstack([normal, anomalies])\n",
    "    y_true = np.array([0] * n_normal + [1] * n_anomaly)\n",
    "    return X , y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmeans anomaly detection scores for n_clusters=1 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99     23371\n",
      "           1       0.48      0.92      0.64       629\n",
      "\n",
      "    accuracy                           0.97     24000\n",
      "   macro avg       0.74      0.95      0.81     24000\n",
      "weighted avg       0.98      0.97      0.98     24000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "df = generate_tram_data(5)\n",
    "y = df['anomaly']\n",
    "X = df[['speed', 'vibration', 'temperature']]\n",
    "\n",
    "# X, y = generate_sensor_data()\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, y_train, X_test, y_test = train_test_split(X_scaled, y, shuffle=True, random_state=42)\n",
    "\n",
    "def kmeans_anomaly_detection(X, y, percentile=95):\n",
    "    param_grid = {\n",
    "        \"n_clusters\": [1, 2, 5, 10, 20, 30, 50, 100]\n",
    "    }\n",
    "    kmeans = KMeans()\n",
    "    grid = GridSearchCV(\n",
    "        estimator=kmeans,\n",
    "        param_grid=param_grid,\n",
    "        scoring='f1_macro'\n",
    "    )\n",
    "\n",
    "    grid.fit(X, y)\n",
    "    n_clusters = grid.best_params_['n_clusters']\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10)\n",
    "    kmeans.fit(X)\n",
    "\n",
    "    distances = np.array([np.linalg.norm(X - center, axis=1) for center in kmeans.cluster_centers_])    \n",
    "    min_distances = np.min(distances, axis=0)\n",
    "    threshold = np.percentile(min_distances, percentile)\n",
    "    anomalies = (distances > threshold).astype(int)\n",
    "    y_anomalies = []\n",
    "    for i, l in enumerate(kmeans.labels_):\n",
    "        y_anomalies.append(anomalies[l, i])\n",
    "\n",
    "    print(f\"kmeans anomaly detection scores for n_clusters={n_clusters} \\n {classification_report(y, y_anomalies)}\")\n",
    "\n",
    "kmeans_anomaly_detection(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbscan anomaly detection scores for eps=2.0, minPts=10 \n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     23371\n",
      "           1       0.66      0.97      0.79       629\n",
      "\n",
      "    accuracy                           0.99     24000\n",
      "   macro avg       0.83      0.98      0.89     24000\n",
      "weighted avg       0.99      0.99      0.99     24000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def dbscan_anomaly_prediction(X, y, eps, min_samples):\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    db.fit(X)\n",
    "\n",
    "    y_anomalies = [1 if l==-1 else 0 for l in db.labels_]\n",
    "    print(f\"dbscan anomaly detection scores for eps={eps}, minPts={min_samples} \\n \\\n",
    "          {classification_report(y, y_anomalies)}\")\n",
    "    \n",
    "dbscan_anomaly_prediction(X, y, 2.0, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lof anomaly detection scores for n_neighbors=1200 \n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     23371\n",
      "           1       0.62      1.00      0.76       629\n",
      "\n",
      "    accuracy                           0.98     24000\n",
      "   macro avg       0.81      0.99      0.88     24000\n",
      "weighted avg       0.99      0.98      0.99     24000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "def lof_anomalies_predict(X, y, n_neighbors):\n",
    "    lof = LocalOutlierFactor(n_neighbors=n_neighbors)\n",
    "    anomalies = lof.fit_predict(X)\n",
    "\n",
    "    y_anomalies = [1 if a == -1 else 0 for a in anomalies]\n",
    "    print(f\"lof anomaly detection scores for n_neighbors={n_neighbors} \\n \\\n",
    "          {classification_report(y, y_anomalies)}\")\n",
    "    \n",
    "lof_anomalies_predict(X, y, 1200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_list2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
